{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import contextlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gpt2 model and tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token to be the same as the EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Loaded {model_name} model and tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare a standard dataset\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load a subset of the WikiText-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1000]\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working on new copy here\n",
    "\n",
    "\n",
    "#Jacobian of (Q_l_plus[subsetting_to_new_channel] . (layer) . Q_l( channel_input, with rest of layer as dummy))\n",
    "#Why do jacobian?\n",
    "#We care about the block-diagonal of the jacobian from rotated-basis to rotated-basis\n",
    "#This all computes 1 of the num_channels diagonal blocks\n",
    "#Need to loop over k\n",
    "\n",
    "#What do we hope we see?\n",
    "#We hope that we observe circuits localize inside channels\n",
    "#Train this on IOI task data\n",
    "#Does the IOI task hop inside a single channel?\n",
    "\n",
    "#We probably have to fix the channel computaton!\n",
    "from torch.func import functional_call, jacrev, vmap, jvp, vjp\n",
    "\n",
    "\n",
    "def stochastic_jvp(current_rotated_layer, random_vector, channel_start, channel_end, ortho_l, ortho_l_plus):\n",
    "    channel = current_rotated_layer[:, channel_start:channel_end]\n",
    "    dummy_layer = current_rotated_layer.detach() \n",
    "    # Compute JVP\n",
    "    #Must check shape!\n",
    "    _, jvp_result = jvp(lambda x: push_forward_channel_with_dummies_outside(x, dummy_layer, channel_start, channel_end, ortho_l, ortho_l_plus), (channel,), (random_vector,))\n",
    "    return jvp_result.unsqueeze(0)\n",
    "\n",
    "\n",
    "def push_forward_channel_with_dummies_outside(channel, dummy_layer, channel_start, channel_end, ortho_l, ortho_l_plus):\n",
    "        \n",
    "        dummied_layer_with_channel_rotated = torch.cat([dummy_layer[:, :channel_start], channel, dummy_layer[:, channel_end:]], dim=-1)\n",
    "    \n",
    "          #result should be  batch, seq_len, d_channel\n",
    "\n",
    "        dummied_layer_with_channel =  (ortho_l.transpose(-2, -1) @ dummied_layer_with_channel_rotated.unsqueeze(-1)).squeeze(-1)\n",
    "        next_layer = model.transformer.h[layer](dummied_layer_with_channel)[0]\n",
    "        # next_layer_rotated = (ortho_l_plus @ next_layer.unsqueeze(-1)).squeeze(-1)\n",
    "        # return next_layer_rotated[:, :, channel_start:channel_end]\n",
    "        next_layer_channel = (ortho_l_plus[:,:,channel_start:channel_end,:] @ next_layer.unsqueeze(-1)).squeeze(-1)\n",
    "        return next_layer_channel #should have shape: batch, seq, channel_width\n",
    "\n",
    "\n",
    "def compute_stochastic_channel_penalty(layer, k, channel_width, ortho_l, ortho_l_plus, activations_l, use_only_jacobian_to_measure_influence = True, num_samples=10):\n",
    "    batch_size, seq_len, d_embed = activations_l.shape\n",
    "\n",
    "    # Pre-compute random vectors\n",
    "    random_vectors = torch.randint(0, 2, (batch_size, seq_len, channel_width), device=activations_l.device) * 2 - 1\n",
    "    random_vectors = random_vectors.float()\n",
    "\n",
    "    ortho_l = ortho_l.unsqueeze(0).unsqueeze(0) #dims: 1, 1, 768, 768\n",
    "    ortho_l_plus = ortho_l_plus.unsqueeze(0).unsqueeze(0) #dims: 1, 1, 768, 768\n",
    "\n",
    "    #rotate to the new basis\n",
    "    rotated_l = (ortho_l @ activations_l.unsqueeze(-1)).squeeze(-1)\n",
    "    channel_start = k * channel_width\n",
    "    channel_end = (k+1) * channel_width\n",
    "\n",
    "    # Compute stochastic JVP for multiple samples\n",
    " \n",
    "    stochastic_jvps = vmap(stochastic_jvp, in_dims=(0, 0, None, None, None, None))(rotated_l, random_vectors, channel_start, channel_end, ortho_l, ortho_l_plus)\n",
    "    \n",
    "    # Compute the penalty using the stochastic JVPs\n",
    "    penalty = torch.mean(torch.sum(stochastic_jvps ** 2, dim=(1, 2, 3, 4)))\n",
    "\n",
    "    if use_only_jacobian_to_measure_influence:\n",
    "        return penalty, stochastic_jvps\n",
    "\n",
    "    if not use_only_jacobian_to_measure_influence:\n",
    "        rotated_clean = (ortho_l.transpose(-2,-1) @ average_activations[layer].unsqueeze(-1)).squeeze(-1)\n",
    "        delta_activation = rotated_l[:, :, channel_start:channel_end] - rotated_clean[:, :seq_len, channel_start:channel_end]\n",
    "        # Note: We can't compute implied_self_channel_influence as before since we don't have the full Jacobian\n",
    "        # Instead, we'll use the average of our stochastic estimates\n",
    "        implied_self_channel_influence = torch.mean(torch.sum(stochastic_jvps * delta_activation.unsqueeze(0).unsqueeze(2), dim=(3, 4)), dim=0)\n",
    "        return torch.sum(implied_self_channel_influence ** 2), stochastic_jvps, delta_activation\n",
    "\n",
    "\n",
    "    #Penalty is the first item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel penalty for layer 6, channel 2: 0.6883862018585205\n",
      "Jacobian shape: torch.Size([2, 1, 1, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "#Testing for stochastic jvp\n",
    "# Set up parameters\n",
    "layer = 6\n",
    "channel_width = 8\n",
    "k = 2\n",
    "hidden_size = 768  # GPT2 hidden size\n",
    "\n",
    "# Create orthogonal matrices for layers 6 and 7\n",
    "ortho_l = torch.nn.init.orthogonal_(torch.empty(hidden_size, hidden_size)).to(device)\n",
    "ortho_l_plus = torch.nn.init.orthogonal_(torch.empty(hidden_size, hidden_size)).to(device)\n",
    "\n",
    "# Generate random input for testing\n",
    "batch_size = 2  # For simplicity, we'll use batch size of 1\n",
    "seq_length = 10\n",
    "input_ids = torch.randint(0, 50257, (batch_size, seq_length)).to(device)\n",
    "\n",
    "# Get model activations for layer 6\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, output_hidden_states=True)\n",
    "    activations_l = outputs.hidden_states[layer]\n",
    "\n",
    "# # Compute average activations (you might want to do this over a larger dataset)\n",
    "# with torch.no_grad():\n",
    "#     average_activations = [torch.mean(act, dim=(0, 1)) for act in outputs.hidden_states]\n",
    "\n",
    "# Compute the channel penalty\n",
    "penalty, jacobian = compute_stochastic_channel_penalty(layer, k, channel_width, ortho_l, ortho_l_plus, activations_l)\n",
    "\n",
    "print(f\"Channel penalty for layer {layer}, channel {k}: {penalty.item()}\")\n",
    "print(f\"Jacobian shape: {jacobian.shape}\")\n",
    "#print(f\"Delta activation shape: {delta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel penalty for layer 6, channel 2: 0.6095150113105774\n",
      "Jacobian shape: torch.Size([2, 1, 1, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# Set up parameters\n",
    "layer = 6\n",
    "channel_width = 8\n",
    "k = 2\n",
    "hidden_size = 768  # GPT2 hidden size\n",
    "\n",
    "# Create orthogonal matrices for layers 6 and 7\n",
    "ortho_l = torch.nn.init.orthogonal_(torch.empty(hidden_size, hidden_size)).to(device)\n",
    "ortho_l_plus = torch.nn.init.orthogonal_(torch.empty(hidden_size, hidden_size)).to(device)\n",
    "\n",
    "# Generate random input for testing\n",
    "batch_size = 2  # For simplicity, we'll use batch size of 1\n",
    "seq_length = 10\n",
    "input_ids = torch.randint(0, 50257, (batch_size, seq_length)).to(device)\n",
    "\n",
    "# Get model activations for layer 6\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, output_hidden_states=True)\n",
    "    activations_l = outputs.hidden_states[layer]\n",
    "\n",
    "# # Compute average activations (you might want to do this over a larger dataset)\n",
    "# with torch.no_grad():\n",
    "#     average_activations = [torch.mean(act, dim=(0, 1)) for act in outputs.hidden_states]\n",
    "\n",
    "# Compute the channel penalty\n",
    "penalty, jacobian = compute_stochastic_channel_penalty(layer, k, channel_width, ortho_l, ortho_l_plus, activations_l)\n",
    "\n",
    "print(f\"Channel penalty for layer {layer}, channel {k}: {penalty.item()}\")\n",
    "print(f\"Jacobian shape: {jacobian.shape}\")\n",
    "#print(f\"Delta activation shape: {delta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 1:\n",
      "  Time: 0.0145 seconds\n",
      "  Penalty: 701.474426\n",
      "\n",
      "Batch size 2:\n",
      "  Time: 0.0122 seconds\n",
      "  Penalty: 700.664124\n",
      "\n",
      "Batch size 8:\n",
      "  Time: 0.0128 seconds\n",
      "  Penalty: 706.942749\n",
      "\n",
      "Batch size 16:\n",
      "  Time: 0.0146 seconds\n",
      "  Penalty: 714.234375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def time_compute_channel_penalty(batch_sizes, seq_length, layer, channel_width, k, ortho_l, ortho_l_plus, model, device):\n",
    "    results = {}\n",
    "    for batch_size in batch_sizes:\n",
    "        # Generate random input\n",
    "        input_ids = torch.randint(0, 50257, (batch_size, seq_length)).to(device)\n",
    "\n",
    "        # Compute activations for the batch in layer 6\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, output_hidden_states=True)\n",
    "            activations = outputs.hidden_states[layer]\n",
    "        \n",
    "        # # Center the activations [this already happens inside the function call]\n",
    "        # mean_activation = torch.mean(activations, dim=(0, 1), keepdim=True)\n",
    "        # centered_activations = activations - mean_activation\n",
    "        \n",
    "        # Time the computation\n",
    "        start_time = time.time()\n",
    "        penalty, _, = compute_stochastic_channel_penalty(layer, k, channel_width, ortho_l, ortho_l_plus, activations)\n",
    "        #penalty, _, = compute_channel_penalty(layer, k, channel_width, ortho_l, ortho_l_plus, activations)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        results[batch_size] = {\n",
    "            'time': end_time - start_time,\n",
    "            'penalty': penalty.item()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Set up parameters\n",
    "layer = 6\n",
    "channel_width = 64\n",
    "k = 2\n",
    "#batch_sizes = [1, 2, 16]\n",
    "batch_sizes = [1, 2, 8, 16]\n",
    "seq_length = 128\n",
    "\n",
    "# Time the function for different batch sizes\n",
    "timing_results = time_compute_channel_penalty(\n",
    "    batch_sizes, seq_length, layer, channel_width, k, \n",
    "    ortho_l, ortho_l_plus, model, device\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for batch_size, result in timing_results.items():\n",
    "    print(f\"Batch size {batch_size}:\")\n",
    "    print(f\"  Time: {result['time']:.4f} seconds\")\n",
    "    print(f\"  Penalty: {result['penalty']:.6f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 12 orthogonal matrices, one for each layer.\n",
      "Each matrix shape: torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "#Now, let's prepare to do this for all layers!\n",
    "#First initialize an orthogonal matrix for every layer of gpt-2\n",
    "\n",
    "#import torch.nn.utils.parametrize as P\n",
    "\n",
    "num_layers = 12  # GPT-2 small has 12 layers\n",
    "hidden_size = 768  # GPT-2 small has a hidden size of 768\n",
    "\n",
    "# Initialize a list to store orthogonal matrices for each layer\n",
    "class OrthogonalMatrix(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        # Initialize the parameter\n",
    "        self.Q = nn.Parameter(torch.empty(n, n))\n",
    "        nn.init.eye_(self.Q)  # Initialize as identity matrix for stability\n",
    "        # Register the orthogonal parametrization\n",
    "        torch.nn.utils.parametrizations.orthogonal(self, 'Q')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This example assumes you will use the orthogonal matrix to transform an input x\n",
    "        return self.Q @ x\n",
    "\n",
    "# Create a list of orthogonal matrices for each layer\n",
    "orthogonal_matrices = nn.ModuleList([OrthogonalMatrix(hidden_size) for _ in range(num_layers)])\n",
    "\n",
    "# Move the matrices to the same device as the model\n",
    "orthogonal_matrices = orthogonal_matrices.to(device)\n",
    "\n",
    "# Print to confirm the creation\n",
    "print(f\"Created {len(orthogonal_matrices)} orthogonal matrices, one for each layer.\")\n",
    "print(f\"Each matrix shape: {orthogonal_matrices[0].Q.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization parameters of GPT-2 have been turned off.\n",
      "All GPT-2 parameters frozen: True\n"
     ]
    }
   ],
   "source": [
    "# Turn off optimization parameters of GPT-2\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Optimization parameters of GPT-2 have been turned off.\")\n",
    "# Verify that all parameters are frozen\n",
    "all_frozen = all(not p.requires_grad for p in model.parameters())\n",
    "print(f\"All GPT-2 parameters frozen: {all_frozen}\")\n",
    "\n",
    "#meanwhile, orthogonal_matrices should be optimizable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed hidden activations for 12 layers\n",
      "Shape of hidden activations for first layer: torch.Size([16, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "#Load test data\n",
    "for batch in dataloader:\n",
    "    break\n",
    "\n",
    "# Compute hidden activations at all layers from the batch\n",
    "inputs = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "# Get the hidden states for all layers\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs, attention_mask=attention_mask, output_hidden_states=True)\n",
    "# Extract all hidden states\n",
    "all_hidden_states = outputs.hidden_states\n",
    "# Remove the first element (embedding layer output) and convert to a list of tensors\n",
    "hidden_activations = [hs for hs in all_hidden_states[1:]]\n",
    "\n",
    "print(f\"Computed hidden activations for {len(hidden_activations)} layers\")\n",
    "print(f\"Shape of hidden activations for first layer: {hidden_activations[0].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding orthogonal matrix support\n",
    "class RotateLayer(torch.nn.Module):\n",
    "    \"\"\"A linear transformation with orthogonal initialization.\"\"\"\n",
    "\n",
    "    def __init__(self, n, init_orth=True):\n",
    "        super().__init__()\n",
    "        weight = torch.empty(n, n)\n",
    "        # we don't need init if the saved checkpoint has a nice\n",
    "        # starting point already.\n",
    "        # you can also study this if you want, but it is our focus.\n",
    "        if init_orth:\n",
    "            torch.nn.init.orthogonal_(weight)\n",
    "        self.weight = torch.nn.Parameter(weight, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x.to(self.weight.dtype), self.weight)\n",
    "\n",
    "class RotatedSpaceIntervention(TrainableIntervention, DistributedRepresentationIntervention):\n",
    "\n",
    "    \"\"\"Intervention in the rotated space.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        rotate_layer = RotateLayer(self.embed_dim)\n",
    "        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 64 #number of channels\n",
    "channel_width = hidden_size // K\n",
    "#Now, writing a function to compute the channel penalty, summing over the K channels\n",
    "\n",
    "def compute_layer_penalty(layer): #implicit: K, channel_width, ortho_l, ortho_l_plus, activations_l\n",
    "    penalties = torch.zeros(K)\n",
    "    for k in range(K):\n",
    "        penalties[k], _, = compute_stochastic_channel_penalty(layer, k, channel_width, \n",
    "                                                         orthogonal_matrices[layer].Q, orthogonal_matrices[layer + 1].Q, \n",
    "                                                         hidden_activations[layer])\n",
    "    return penalties\n",
    "\n",
    "def compute_all_layer_penalties():\n",
    "    layer_penalty = torch.zeros(num_layers - 1, K, device = device)\n",
    "    for layer in range(num_layers - 1):\n",
    "        layer_penalty[layer] = compute_layer_penalty(layer)\n",
    "    return torch.sum(layer_penalty ** 2), layer_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelbsklar\u001b[0m (\u001b[33mmichaelsklar\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/aiplay/channeling/wandb/run-20240913_085037-a54oz3bk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelsklar/channel-gpt2-model/runs/a54oz3bk' target=\"_blank\">optimization-run</a></strong> to <a href='https://wandb.ai/michaelsklar/channel-gpt2-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelsklar/channel-gpt2-model' target=\"_blank\">https://wandb.ai/michaelsklar/channel-gpt2-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelsklar/channel-gpt2-model/runs/a54oz3bk' target=\"_blank\">https://wandb.ai/michaelsklar/channel-gpt2-model/runs/a54oz3bk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/michaelsklar/channel-gpt2-model/runs/a54oz3bk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd04fd28e50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Initialize wandb\n",
    "# import wandb\n",
    "# wandb.init(project=\"channel-gpt2-model\", name=\"optimization-run\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "optimizer can only optimize Tensors, but one of the params is torch.nn.utils.parametrize.ParametrizedOrthogonalMatrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set up optimizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43morthogonal_matrices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Number of optimization steps\u001b[39;00m\n\u001b[1;32m      5\u001b[0m num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:883\u001b[0m, in \u001b[0;36mOptimizer.add_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 883\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer can only optimize Tensors, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut one of the params is \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtypename(param))\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mis_leaf \u001b[38;5;129;01mor\u001b[39;00m param\u001b[38;5;241m.\u001b[39mretains_grad):\n\u001b[1;32m    886\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt optimize a non-leaf Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: optimizer can only optimize Tensors, but one of the params is torch.nn.utils.parametrize.ParametrizedOrthogonalMatrix"
     ]
    }
   ],
   "source": [
    "# Set up optimizer\n",
    "optimizer = torch.optim.Adam([orthogonal_matrices[i] for i in range(num_layers)], lr=0.001)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss, layer_penalties = compute_all_layer_penalties()\n",
    "    \n",
    "    # Backpropagate the loss\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the orthogonal matrices\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log the loss to wandb\n",
    "    wandb.log({\"loss\": loss.item()})\n",
    "    \n",
    "    # Print progress\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"Step {step + 1}/{num_steps}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Optimization complete.\")\n",
    "\n",
    "# Compute final penalties\n",
    "final_loss, final_layer_penalties = compute_all_layer_penalties()\n",
    "print(f\"Final Loss: {final_loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final layer penalties\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(final_layer_penalties.cpu().detach().numpy(), aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Penalty')\n",
    "plt.xlabel('Channel')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Final Layer Penalties')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
